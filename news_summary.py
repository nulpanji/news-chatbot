import streamlit as st
import requests
from deep_translator import GoogleTranslator
import datetime

NEWSAPI_KEY = "f2f31ac43bcd4f7aab46adf98f73b8dd"
GNEWS_API_KEY = "3f54020e7158efbf628c9c7227bbdd0f"


def fetch_gnews_articles(keyword, translate_to_ko=False, lang="ko", max_articles=10):
    # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬
    from deep_translator import GoogleTranslator
    
    # í‚¤ì›Œë“œ ë¶„ë¦¬ (ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ)
    # ì˜ˆ: 'í…ŒìŠ¬ë¼ ìµœê·¼ ë‰´ìŠ¤ ì•Œë ¤ì¤˜' -> ['tesla', 'í…ŒìŠ¬ë¼']
    main_keywords = []
    
    # ì›ë³¸ í‚¤ì›Œë“œì—ì„œ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
    important_words = [w.strip() for w in keyword.split() if len(w.strip()) > 1 and w.strip() not in ["ì•Œë ¤ì¤˜", "ì•Œë ¤ì¤„", "ì•Œë ¤ì¤„ê¹Œ", "ì•Œë ¤ì¤„ê¹Œìš”", "ì•Œë ¤ì£¼ì„¸ìš”", "ì•Œë ¤ì£¼ì„¸ìš”", "ì•Œë ¤ì¤ë‹ˆë‹¤", "ì•Œë ¤ì¤ë‹ˆê¹Œ", "ì•Œë ¤ì£¼", "ì•Œë ¤ì¤Œ", "ì•Œë ¤ì¤ë‹ˆë‹¤", "ìµœê·¼", "ê´€ë ¨", "ê¸°ì‚¬", "ë‰´ìŠ¤", "ìš”ì•½", "ìš”ì•½í•´ì¤˜", "ìˆì–´", "ìˆë‚˜ìš”", "ìˆì–´ìš”", "ìˆë‚˜", "ìˆë‚˜ìš”", "ìˆë‚˜ìš”?", "ë­", "ë­ê°€", "ë­ê°€ ìˆì–´", "ë­ê°€ ìˆì–´ìš”", "ë­ê°€ ìˆë‚˜ìš”"]]
    
    # ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
    for word in important_words:
        main_keywords.append(word)
    
    # ì˜ì–´ ë²ˆì—­ ì‹œë„
    try:
        # ì „ì²´ ë¬¸ì¥ ë²ˆì—­
        keyword_en = GoogleTranslator(source='ko', target='en').translate(keyword)
        # ê°œë³„ í‚¤ì›Œë“œ ë²ˆì—­
        for word in important_words:
            try:
                en_word = GoogleTranslator(source='ko', target='en').translate(word)
                if en_word and en_word.lower() not in [w.lower() for w in main_keywords]:
                    main_keywords.append(en_word)
            except:
                pass
    except Exception:
        keyword_en = keyword
    
    # ê²€ìƒ‰ì–´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
    if not main_keywords:
        main_keywords = [keyword]
    
    print(f"[ê²€ìƒ‰ í‚¤ì›Œë“œ] {main_keywords}")
    
    results = []
    langs = ["ko", "en"] if lang == "all" else [lang]
    
    # ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹œë„
    for use_kw in main_keywords:
        for l in langs:
            url = "https://gnews.io/api/v4/search"
            params = {
                "q": use_kw,
                "lang": l,
                "max": max_articles,
                "token": GNEWS_API_KEY
            }
            try:
                res = requests.get(url, params=params, timeout=10)
                data = res.json()
                print(f"[GNews][{l}] '{use_kw}' totalArticles:", data.get("totalArticles", 0), "/ articles:", len(data.get("articles", [])))
                
                for art in data.get("articles", []):
                    title = art.get("title", "") or ""
                    summary = art.get("description", "") or ""
                    link = art.get("url", "")
                    source = art.get("source", {}).get("name", "")
                    pub_date = art.get("publishedAt", "")[:16].replace("T", " ")
                    
                    # ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ ê³ ìœ  ID ìƒì„±
                    article_id = f"{link}_{title[:20]}"
                    
                    # ë²ˆì—­ ì²˜ë¦¬
                    if translate_to_ko and l != "ko":
                        try:
                            title_ko = GoogleTranslator(source='auto', target='ko').translate(title) if title else title
                        except Exception:
                            title_ko = title
                        try:
                            summary_ko = GoogleTranslator(source='auto', target='ko').translate(summary) if summary else summary
                        except Exception:
                            summary_ko = summary
                    else:
                        title_ko = title
                        summary_ko = summary
                    
                    # ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€
                    results.append({
                        "id": article_id,
                        "title": title_ko,
                        "summary": summary_ko,
                        "link": link,
                        "source": source,
                        "pub_date": pub_date,
                        "keyword": use_kw
                    })
            except Exception as e:
                print(f"[GNews] ì˜¤ë¥˜ {use_kw}: {e}")
    
    # ì¤‘ë³µ ì œê±° (ê³ ìœ  ID ê¸°ë°˜)
    unique_results = []
    seen_ids = set()
    
    for article in results:
        if article["id"] not in seen_ids:
            seen_ids.add(article["id"])
            unique_results.append(article)
    
    return unique_results, len(unique_results)

# NewsAPIë¡œ í‚¤ì›Œë“œ ë‰´ìŠ¤ ê²€ìƒ‰ (ìµœëŒ€ 100ê°œ, ì–¸ì–´: ì˜ì–´/í•œêµ­ì–´/ë‹¤êµ­ì–´)
def fetch_newsapi_articles(keyword, translate_to_ko=False):
    from deep_translator import GoogleTranslator
    try:
        keyword_en = GoogleTranslator(source='ko', target='en').translate(keyword)
    except Exception:
        keyword_en = keyword
    results = []
    for use_kw in [keyword, keyword_en]:
        url = "https://newsapi.org/v2/everything"
        params = {
            "q": use_kw,
            "from": (datetime.datetime.utcnow() - datetime.timedelta(days=30)).strftime("%Y-%m-%d"),
            "sortBy": "publishedAt",
            "language": "all",  # ko/en/all ëª¨ë‘ ì‹œë„
            "pageSize": 100,
            "apiKey": NEWSAPI_KEY,
        }
        res = requests.get(url, params=params, timeout=10)
        data = res.json()
        print(f"[NewsAPI] '{use_kw}' totalResults:", data.get("totalResults"), "/ articles:", len(data.get("articles", [])))
        keywords = [k.strip() for k in keyword.split() if k.strip()]
        for art in data.get("articles", []):
            title = art["title"] or ""
            summary = art["description"] or ""
            link = art["url"]
            source = art["source"]["name"]
            pub_date = art["publishedAt"][:16].replace("T", " ")
            text = (title + " " + summary).lower()
            # OR ê²€ìƒ‰: í•˜ë‚˜ë¼ë„ í¬í•¨
            if any(k.lower() in text for k in keywords):
                if translate_to_ko:
                    try:
                        title_ko = GoogleTranslator(source='auto', target='ko').translate(title) if title else title
                    except Exception:
                        title_ko = title
                    try:
                        summary_ko = GoogleTranslator(source='auto', target='ko').translate(summary) if summary else summary
                    except Exception:
                        summary_ko = summary
                else:
                    title_ko = title
                    summary_ko = summary
                results.append({
                    "title": title_ko,
                    "summary": summary_ko,
                    "link": link,
                    "source": source,
                    "pub_date": pub_date
                })
    return results, len(results)

# ì‹¤ì‹œê°„ HOT ë‰´ìŠ¤ ì¶”ì¶œ - ì„ ì§„êµ­ ì¤‘ì‹¬ ì¸ê¸° ë‰´ìŠ¤ ê¸°ë°˜
def fetch_hot_news():
    import datetime
    # ì£¼ìš” ì„ ì§„êµ­ ëª©ë¡
    countries = ['us', 'gb', 'jp', 'kr', 'sg', 'de', 'fr', 'ca', 'au']
    all_articles = []
    
    # 1. NewsAPIì˜ Top Headlines ì‚¬ìš© (êµ­ê°€ë³„ í—¤ë“œë¼ì¸)
    for country in countries:
        url = "https://newsapi.org/v2/top-headlines"
        params = {
            "country": country,
            "pageSize": 10,  # êµ­ê°€ë‹¹ ìµœëŒ€ 10ê°œ
            "apiKey": NEWSAPI_KEY
        }
        try:
            res = requests.get(url, params=params, timeout=10)
            data = res.json()
            if data.get("status") == "ok":
                articles = data.get("articles", [])
                print(f"[{country.upper()} í—¤ë“œë¼ì¸] {len(articles)}ê°œ ê¸°ì‚¬ ê°€ì ¸ì˜´")
                for art in articles:
                    title = art.get("title", "")
                    summary = art.get("description", "")
                    link = art.get("url", "")
                    source = art.get("source", {}).get("name", "")
                    pub_date = art.get("publishedAt", "")[:16].replace("T", " ")
                    all_articles.append({
                        "title": title,
                        "summary": summary,
                        "link": link,
                        "source": source,
                        "pub_date": pub_date,
                        "country": country.upper(),
                        "type": "headline"
                    })
        except Exception as e:
            print(f"[{country}] ì˜¤ë¥˜: {e}")
    
    # 2. NewsAPIì˜ Everything ì—ì„œ ì¸ê¸°ë„ ê¸°ë°˜ ê²€ìƒ‰
    url = "https://newsapi.org/v2/everything"
    from_date = (datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=3)).strftime("%Y-%m-%d")
    # ì£¼ìš” í† í”½ í‚¤ì›Œë“œ
    topics = [
        "global economy", "international politics", "technology innovation", 
        "climate change", "health crisis", "breaking news", "major sports"
    ]
    
    for topic in topics:
        params = {
            "q": topic,
            "from": from_date,
            "sortBy": "popularity",  # ì¸ê¸°ë„ ê¸°ë°˜ ì •ë ¬
            "language": "en",
            "pageSize": 5,  # í† í”½ë‹¹ 5ê°œ
            "apiKey": NEWSAPI_KEY
        }
        try:
            res = requests.get(url, params=params, timeout=10)
            data = res.json()
            if data.get("status") == "ok":
                articles = data.get("articles", [])
                print(f"[{topic}] {len(articles)}ê°œ ì¸ê¸° ê¸°ì‚¬ ê°€ì ¸ì˜´")
                for art in articles:
                    title = art.get("title", "")
                    summary = art.get("description", "")
                    link = art.get("url", "")
                    source = art.get("source", {}).get("name", "")
                    pub_date = art.get("publishedAt", "")[:16].replace("T", " ")
                    all_articles.append({
                        "title": title,
                        "summary": summary,
                        "link": link,
                        "source": source,
                        "pub_date": pub_date,
                        "topic": topic,
                        "type": "popular"
                    })
        except Exception as e:
            print(f"[{topic}] ì˜¤ë¥˜: {e}")
    
    # 3. ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ë°˜)
    unique_articles = []
    seen_titles = set()
    
    for article in all_articles:
        # ì œëª© ì •ë¦¬ (ê³µë°± ì œê±°, ì†Œë¬¸ìí™”)
        # ì œëª©ì´ Noneì´ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        title = article.get("title", "")
        if title is None:
            title = ""
        normalized_title = title.lower().strip()
        if normalized_title and normalized_title not in seen_titles:
            seen_titles.add(normalized_title)
            unique_articles.append(article)
    
    print(f"[ì´ê³„] ì¤‘ë³µ ì œê±° í›„ {len(unique_articles)}ê°œ ê¸°ì‚¬")
    
    # 4. ìµœì‹ ìˆœ + ì¸ê¸°ë„ ê¸°ì¤€ í˜¼í•© ì •ë ¬
    # í—¤ë“œë¼ì¸ì€ ìš°ì„ ìˆœìœ„, ë‚˜ë¨¸ì§€ëŠ” ìµœì‹ ìˆœ
    headlines = [a for a in unique_articles if a.get("type") == "headline"]
    popular_articles = [a for a in unique_articles if a.get("type") == "popular"]
    
    # ìµœì‹ ìˆœ ì •ë ¬
    headlines.sort(key=lambda x: x["pub_date"], reverse=True)
    popular_articles.sort(key=lambda x: x["pub_date"], reverse=True)
    
    # í—¤ë“œë¼ì¸ ë¨¼ì €, ë‚˜ë¨¸ì§€ ì¸ê¸°ê¸°ì‚¬ ë‚˜ì¤‘
    final_articles = headlines + popular_articles
    
    # ìµœëŒ€ 30ê°œë§Œ ë°˜í™˜
    return final_articles[:30]


# ì‹¤ì‹œê°„ ì£¼ì‹ ì‹œì„¸ (Yahoo Finance)


def fetch_and_translate_news(keyword=None, translate_to_ko=False):
    articles = fetch_newsapi_articles(keyword, translate_to_ko=translate_to_ko)
    now = datetime.datetime.utcnow()
    one_month_ago = now - datetime.timedelta(days=31)
    all_feeds = sum(RSS_FEEDS.values(), [])
    for url in all_feeds:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            # ë‚ ì§œ íŒŒì‹±
            pub_date = None
            for date_key in ['published_parsed', 'updated_parsed']:
                if date_key in entry and entry[date_key]:
                    pub_date = datetime.datetime(*entry[date_key][:6])
                    break
            if not pub_date:
                continue
            # í•œ ë‹¬ ì´ë‚´ë§Œ
            if pub_date < one_month_ago:
                continue
            title = entry.title
            # content ìš°ì„ , ì—†ìœ¼ë©´ summary
            if 'content' in entry and entry.content:
                summary = entry.content[0].value
            elif 'summary' in entry:
                summary = entry.summary
            else:
                summary = ''
            link = entry.link
            if translate_to_ko:
                try:
                    title_ko = translator.translate(title, dest='ko').text
                except Exception:
                    title_ko = title
                try:
                    summary_ko = translator.translate(summary, dest='ko').text if summary else summary
                except Exception:
                    summary_ko = summary
            else:
                title_ko = title
                summary_ko = summary
            # í‚¤ì›Œë“œ í•„í„° (ì›ë¬¸+ë²ˆì—­ ëª¨ë‘ í¬í•¨)
            if keyword:
                keyword_lower = keyword.lower()
                if (keyword_lower not in title.lower() and
                    keyword_lower not in summary.lower() and
                    keyword_lower not in title_ko.lower() and
                    keyword_lower not in summary_ko.lower()):
                    continue
            articles.append({
                'title': title_ko,
                'summary': summary_ko,
                'link': link,
                'source': feed.feed.title if 'title' in feed.feed else url,
                'pub_date': pub_date
            })
    # ìµœì‹ ìˆœ ì •ë ¬ (ë§¨ ìœ„ê°€ ìµœì‹ )
    articles.sort(key=lambda x: x['pub_date'], reverse=True)
    return articles

# Streamlit ì›¹ì±—ë´‡ UI
# ---- UI ----
st.set_page_config(page_title="ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì±—ë´‡", layout="wide")

# ë¸Œë¼ìš°ì € ìº ì‹œ ë°©ì§€ë¥¼ ìœ„í•œ ì½”ë“œ
st.markdown("""
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
""", unsafe_allow_html=True)

st.markdown("""
<style>
    .main {max-width: 700px; margin: auto;}
    @media (max-width: 600px) {
        .main {max-width: 100vw; padding: 0 8px;}
    }
</style>
<div class="main">
""", unsafe_allow_html=True)

st.title("ğŸŒ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì±—ë´‡")


# ---- ë‰´ìŠ¤ ê²€ìƒ‰ UI (ì±—ë´‡ ìŠ¤íƒ€ì¼) ----
st.markdown("""
<div style='display:flex; align-items:center; justify-content:center; gap:10px;'>
    <span style='font-size:2em;'>ğŸ“°ğŸ¤–</span>
    <span style='font-size:1.5em; font-weight:bold;'>ë‰´ìŠ¤ ëŒ€í™”í˜• ì±—ë´‡</span>
    <span style='font-size:2em;'>ğŸ”</span>
</div>
""", unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì‚¬ ì–¸ì–´ ì„ íƒ ë©”ë‰´ (ê²€ìƒ‰ì°½ ìœ„)
lang_option = st.radio("ê¸°ì‚¬ ì–¸ì–´ ì„ íƒ", ["ì›ë³¸", "ëª¨ë“  ê¸°ì‚¬ í•œêµ­ì–´ë¡œ ë²ˆì—­"], horizontal=True)
translate_to_ko = lang_option == "ëª¨ë“  ê¸°ì‚¬ í•œêµ­ì–´ë¡œ ë²ˆì—­"

# ì±—ë´‡ ìŠ¤íƒ€ì¼ ê²€ìƒ‰ ì…ë ¥ì°½ (ë‡ë³´ê¸° ì•„ì´ì½˜, ì—”í„°ë¡œ ê²€ìƒ‰)
with st.form(key="chatbot_form", clear_on_submit=True):
    col1, col2 = st.columns([20,1])
    user_input = col1.text_input("ê²€ìƒ‰", "", placeholder="ë‰´ìŠ¤, í‚¤ì›Œë“œ, ì¸ë¬¼, ì´ìŠˆ ë“± ììœ ë¡­ê²Œ ë¬»ì–´ë³´ì„¸ìš”!", label_visibility="collapsed")
    submitted = col2.form_submit_button("ğŸ”", use_container_width=True)

# --- HOT ë‰´ìŠ¤ ìœ„ì ¯ ---
hot_news = fetch_hot_news()
with st.expander("ğŸ”¥ ìµœê·¼ ê¸€ë¡œë²Œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸", expanded=True):
    if not hot_news:
        st.info("ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for i, art in enumerate(hot_news, 1):
            # ì œëª©(í•˜ì´í¼ë§í¬)ë§Œ í‘œì‹œ, ìš”ì•½ ë‚´ìš© ì œê±°
            st.markdown(f"**{i}. [{art['title']}]({art['link']})**")
            # ì¶œì²˜ì™€ ë‚ ì§œë§Œ ê°„ëµí•˜ê²Œ í‘œì‹œ
            st.caption(f"{art['source']} | {art['pub_date']}")

# ì˜ˆì‹œ ì§ˆë¬¸ë§Œ ì‚¬ì´ë“œë°”ì— ì•ˆë‚´ (ì‚¬ì´ë“œë°” ì•ˆë‚´ë§Œ ìœ ì§€)
with st.sidebar:
    st.markdown("---")
    st.markdown("**ì˜ˆì‹œ ì§ˆë¬¸:**\n- í…ŒìŠ¬ë¼ ìµœê·¼ ë‰´ìŠ¤ ì•Œë ¤ì¤˜\n- ì‚¼ì„±ì „ì ê¸°ì‚¬ ìš”ì•½í•´ì¤˜\n- ì¼ë¡  ë¨¸ìŠ¤í¬ ê´€ë ¨ ê¸°ì‚¬ ë­ ìˆì–´?\n- ì˜¤ëŠ˜ì˜ ê²½ì œ ë‰´ìŠ¤?")


# ê¸°ì¡´ ëŒ€í™” ë‚´ì—­ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

import openai
import os

# OpenAI API í‚¤ ì„¤ì •
def get_openai_api_key():
    # ì‹œí¬ë¦¿ì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸° ì‹œë„
    api_key = None
    try:
        if "OPENAI_API_KEY" in st.secrets:
            api_key = st.secrets["OPENAI_API_KEY"]
            print("Streamlit secretsì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"Streamlit secrets ì˜¤ë¥˜: {e}")
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸° ì‹œë„
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            print("í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    
    # í‚¤ê°€ ì—†ìœ¼ë©´ ìƒìˆ˜ ê°’ ì‚¬ìš© (ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!)
    if not api_key:
        # ì„ì‹œ í…ŒìŠ¤íŠ¸ìš© API í‚¤ (ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!)
        api_key = "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"
        print("ê¸°ë³¸ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë³€ê²½í•´ì£¼ì„¸ìš”.")
    
    return api_key

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜ - ë‰´ìŠ¤ ì±—ë´‡ì˜ ì—­í• ê³¼ í–‰ë™ ë°©ì‹ ì •ì˜
def get_system_prompt():
    return """ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
1. ì‚¬ìš©ìê°€ íŠ¹ì • í‚¤ì›Œë“œë‚˜ ì£¼ì œì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ë¬»ëŠ”ë‹¤ë©´, ê·¸ ì£¼ì œì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ìš”ì•½í•´ì„œ ì œê³µí•˜ì„¸ìš”.
2. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì‚¬ìš©ìê°€ 'ë” ìì„¸íˆ ì•Œë ¤ì¤˜' ë¼ê³  í•˜ë©´ ì´ì „ ì£¼ì œì— ëŒ€í•´ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
3. ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ ì£¼ì œë‚˜ í‚¤ì›Œë“œë¥¼ ì–¸ê¸‰í•˜ë©´, ê·¸ì— ëŒ€í•œ ìƒˆë¡œìš´ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ì œê³µí•˜ì„¸ìš”.
4. ì‚¬ìš©ìê°€ ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ í•˜ë©´, ìƒˆë¡œìš´ ì£¼ì œë¡œ ì¸ì‹í•˜ê³  ê·¸ì— ë§ëŠ” ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ì œê³µí•˜ì„¸ìš”.
5. ì‚¬ìš©ìê°€ ë‰´ìŠ¤ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì„ í•˜ë©´, ì¼ë°˜ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ë©´ì„œ ë‰´ìŠ¤ ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.
"""

# ëŒ€í™” ë§¥ë½ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def analyze_conversation_context(messages):
    # ë§¥ë½ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
    if len(messages) < 2:
        return None, None
    
    # ì´ì „ ëŒ€í™”ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    last_user_messages = [msg["content"] for msg in messages if msg["role"] == "user"][-3:]
    last_assistant_messages = [msg["content"] for msg in messages if msg["role"] == "assistant"][-3:]
    
    # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ ì£¼ì œ ì¶”ì •
    context_messages = [
        {"role": "system", "content": "ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì¬ ëŒ€í™”ì˜ ì£¼ì œì™€ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. JSON í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”: {\"main_topic\": \"ì£¼ì œ\", \"keywords\": [\"í‚¤ì›Œë“œ1\", \"í‚¤ì›Œë“œ2\"]}"}
    ]
    
    # ì´ì „ ëŒ€í™” ë‚´ìš© ì¶”ê°€
    for user_msg, assistant_msg in zip(last_user_messages, last_assistant_messages):
        if user_msg and assistant_msg:
            context_messages.append({"role": "user", "content": user_msg})
            context_messages.append({"role": "assistant", "content": assistant_msg})
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    context_messages.append({"role": "user", "content": messages[-1]["content"]})
    
    try:
        # GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ ë¶„ì„ (ìµœì‹  API ë²„ì „)
        from openai import OpenAI
        client = OpenAI(api_key=get_openai_api_key())
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=context_messages,
            temperature=0.3,
            max_tokens=150
        )
        context_analysis = response.choices[0].message.content
        
        # JSON íŒŒì‹±
        import json
        import re
        
        # JSON í˜•íƒœë¡œ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', context_analysis, re.DOTALL)
        if json_match:
            context_data = json.loads(json_match.group(0))
            main_topic = context_data.get("main_topic")
            keywords = context_data.get("keywords", [])
            return main_topic, keywords
    except Exception as e:
        print(f"[Context Analysis] ì˜¤ë¥˜: {e}")
    
    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    return None, None

# GPTë¥¼ í†µí•œ ë‰´ìŠ¤ ìš”ì•½ ë° ì‘ë‹µ ìƒì„±
def ask_gpt(messages, news_articles=None):
    from openai import OpenAI
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ìµœì‹  API ë²„ì „)
    client = OpenAI(api_key=get_openai_api_key())
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    gpt_messages = [
        {"role": "system", "content": get_system_prompt()}
    ]
    
    # ì´ì „ ëŒ€í™” ë‚´ìš© ì¶”ê°€
    for msg in messages:
        gpt_messages.append(msg)
    
    # ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ì¶”ê°€
    if news_articles:
        news_context = """ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”:\n\n"""
        
        for i, art in enumerate(news_articles[:5], 1):
            news_context += f"{i}. ì œëª©: {art['title']}\n"
            if art['summary']:
                from bs4 import BeautifulSoup
                clean_summary = BeautifulSoup(art['summary'], "html.parser").get_text()
                news_context += f"   ìš”ì•½: {clean_summary[:200]}...\n"
            news_context += f"   ì¶œì²˜: {art['source']} | ë‚ ì§œ: {art['pub_date']}\n\n"
        
        gpt_messages.append({"role": "system", "content": news_context})
    
    # ì¶”ê°€ ì§€ì¹¨ ì œê³µ
    gpt_messages.append({"role": "system", "content": """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
    1. ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‘ë‹µí•˜ì„¸ìš”.
    2. ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    3. ë§¥ë½ì— ë”°ë¼ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ì—¬ ì‘ë‹µí•˜ì„¸ìš”.
    4. ë§Œì•½ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ëŠ” ê²½ìš°, ì‚¬ìš©ìì—ê²Œ ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ì£¼ì œë¥¼ ì œì•ˆí•˜ì„¸ìš”.
    5. ë§Œì•½ ì‚¬ìš©ìê°€ ë” ìì„¸í•œ ì •ë³´ë‚˜ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ë©´, ì´ì „ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë” ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”.
    """})
    
    # GPT ì‘ë‹µ ìƒì„± (ìµœì‹  API ë²„ì „)
    try:
        # API í‚¤ ë¡œê·¸ ì¶”ê°€ (ê°œë°œìš©)
        api_key = get_openai_api_key()
        print(f"API í‚¤ ì‚¬ìš© ì¤‘: {api_key[:5]}...{api_key[-4:] if api_key else 'None'}")
        
        # í´ë¼ì´ì–¸íŠ¸ ì¬ìƒì„±
        client = OpenAI(api_key=api_key)
        
        # API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=gpt_messages,
            temperature=0.7,
            max_tokens=1024
        )
        
        # ì‘ë‹µ ì¶”ì¶œ
        if hasattr(response.choices[0], 'message') and hasattr(response.choices[0].message, 'content'):
            return response.choices[0].message.content
        else:
            print(f"OpenAI API ì‘ë‹µ êµ¬ì¡° ì˜¤ë¥˜: {response}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. API ì‘ë‹µ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except openai.APIError as e:
        print(f"OpenAI API ì˜¤ë¥˜: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. OpenAI API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)[:100]}. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except openai.APIConnectionError as e:
        print(f"OpenAI API ì—°ê²° ì˜¤ë¥˜: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. API ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except openai.RateLimitError as e:
        print(f"OpenAI API ìš”ìœ¨ ì œí•œ ì˜¤ë¥˜: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except openai.AuthenticationError as e:
        print(f"OpenAI API ì¸ì¦ ì˜¤ë¥˜: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. API í‚¤ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
    except Exception as e:
        print(f"OpenAI API ê¸°íƒ€ ì˜¤ë¥˜: {type(e).__name__}: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

if submitted and user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ëŒ€í™” ë§¥ë½ ë¶„ì„
    main_topic, keywords = None, []
    if len(st.session_state.messages) > 1:
        main_topic, keywords = analyze_conversation_context(st.session_state.messages)
    
    # ë§¥ë½ì´ ì—†ê±°ë‚˜ ìƒˆë¡œìš´ ì£¼ì œì¸ ê²½ìš° í˜„ì¬ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰
    search_query = user_input
    
    # ë§¥ë½ì´ ìˆê³  í˜„ì¬ ì…ë ¥ì´ ì§§ì€ ì¶”ê°€ ì§ˆë¬¸ì´ë©´ ì´ì „ ì£¼ì œ í™œìš©
    if main_topic and len(user_input.strip().split()) <= 5 and not any(kw.lower() in user_input.lower() for kw in ['í…ŒìŠ¬ë¼', 'ì‚¼ì„±', 'ì• í”Œ', 'êµ­ì œ', 'ê²½ì œ', 'ì •ì¹˜', 'ìŠ¤í¬ì¸ ', 'ì—°ì˜ˆ', 'ì½”ë¡œë‚˜', 'ì „ìŸ', 'ê¸°í›„']):
        follow_up_keywords = ["ë”", "ìì„¸íˆ", "êµ¬ì²´ì ìœ¼ë¡œ", "ì–´ë–¤", "ë¬´ì—‡", "ì–¸ì œ", "ì–´ë””ì„œ", "ì™œ", "ì–´ë–»ê²Œ", "ë˜", "ì¶”ê°€ë¡œ", "ë‹¤ë¥¸", "ë‹¤ìŒ"]
        if any(kw in user_input.lower() for kw in follow_up_keywords) and keywords:
            # í›„ì† ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë˜ë©´ ì´ì „ í‚¤ì›Œë“œ í™œìš©
            search_query = main_topic if main_topic else " ".join(keywords[:2])
            print(f"[ë§¥ë½ ë¶„ì„] í›„ì† ì§ˆë¬¸ ê°ì§€: '{user_input}' -> ê²€ìƒ‰ì–´: '{search_query}'")
    
    # ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰
    news_articles, total_news = fetch_gnews_articles(search_query, translate_to_ko=translate_to_ko, lang="ko" if translate_to_ko else "en", max_articles=10)
    
    # GNewsì—ì„œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ NewsAPIë„ ì‹œë„
    if not news_articles:
        news_articles2, total_news2 = fetch_newsapi_articles(search_query, translate_to_ko=translate_to_ko)
        news_articles += news_articles2
    
    # ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        if not news_articles:
            # ë‰´ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° GPTë§Œ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
            gpt_reply = ask_gpt(st.session_state.messages)
            st.markdown(gpt_reply)
        else:
            # ë‰´ìŠ¤ ê¸°ì‚¬ í‘œì‹œ
            for i, art in enumerate(news_articles[:5], 1):
                st.markdown(f"**{i}. [{art['title']}]({art['link']})**")
                if art['summary']:
                    from bs4 import BeautifulSoup
                    clean_summary = BeautifulSoup(art['summary'], "html.parser").get_text()
                    st.write(clean_summary[:150] + ("..." if len(clean_summary) > 150 else ""))
                st.caption(f"{art['source']} | {art['pub_date']}")
            
            # GPTë¥¼ í†µí•œ ì§€ëŠ¥ì  ì‘ë‹µ ìƒì„±
            gpt_reply = ask_gpt(st.session_state.messages, news_articles)
            st.markdown(f"\n\n**í•´ì„ ë° ìš”ì•½:**\n{gpt_reply}")
    
    # ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": gpt_reply})
    
    # UI ìƒˆë¡œê³ ì¹¨
    st.rerun()
